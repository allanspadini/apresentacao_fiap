---
title: "&#8203;"
title-slide-attributes:
    data-background-image: imgs/title.png
    data-background-size: contain
    data-background-opacity: "1.0"
author: "&#8203;"
format: 
    revealjs:
        theme: [default, custom.scss]
        transition: fade
        slide-number: true
        preview-links: true
        incremental: true
        embed-resources: false
        notes: false
        self-contained-math: true
---

## ​

::: column-screen-inset
<iframe src="imgs/apresentacao.html" width="2000px" height="600px">

</iframe>
:::

## Pergunta

**Machine learning** e **deep learning** ainda são relevantes na era das LLMs?

![](imgs/duvida.jpg)

::: notes
Para começar essa apresentação eu vou começar com uma pergunta. **Machine learning** e **deep learning** ainda são relevantes na era das LLMs? Pra responder essa pergunta vamos partir do zero. Eu não vou contar a história do machine learning aqui, mas nós vamos entender qual a categorização dessas coisas e que tipo de problemas que elas são capazes de resolver.
:::

## Qual a organização?

![](imgs/sequencia.svg){fig-align="center",width=270}

::: notes
O termo IA explodiu nos últimos anos e quando as pessoas estão falando de LLMs ou de IA generativa no geral elas utilizam o termo IA. Mas na prática inteligência artificial é uma área bem abrangente que engloba o machine learning, dentro do machine learning nós métodos específicos que envolvem redes neurais e como consequência o deep learning. E as LLMs e geradores de imagem no geral são modelos de deep learning treinados com um propósito específico. Embora hoje em dia com uso de ferramentas, agentes e memória e raciocínio estejam tentando imitar o que seria o comportamento de uma IA uma inteligência artifical mais geral.
:::

## O que é o machine learning?

| Renda Mensal (R\$) | Tempo de Relacionamento (meses) | Risco de Inadimplência |
|-------------------|------------------------------|-----------------------|
| 2.500              | 8                               | Alto                   |
| 4.800              | 36                              | Baixo                  |
| 3.200              | 12                              | Médio                  |
| 8.500              | 60                              | Baixo                  |
| 1.800              | 3                               | Alto                   |
| 5.300              | 24                              | Baixo                  |

::: notes
Mas a essência de tudo é o machine learning. No machine learning nós utilizamos técnicas que permitem que os computadores aprendam sem serem explicitamente programados. Por exemplo, eu não tenho uma regra pré definida que eu consiga aplicar pra resolver o problema dessa tabela de me dizer se o risco de inadinplência de um cliente de um banco é alto ou baixo. Porém, existem algoritmos que se eu passar cada um desses exemplos eles vão ser capazes de começar a identificar padrões e posteriormente fazer a classificação dos clientes quando passar um cliente novo. Ou seja, o algoritmo não apenas memorizar esses exemplos, mas também vai generalizar de forma que quando eu passar um exemplo novo de clientes que tem características um pouquinho diferentes ele também vai conseguir classificar os clientes.
:::

## Subdivisões do Machine learning

-   Aprendizado supervisionado

-   Aprendizado não-supervisionado

-   Aprendizagem por reforço

::: notes
E o machine learning pode ser subdividido em três categorias principais que seriam o Aprendizado supervisionado, Aprendizado não-supervisionado, Aprendizagem por reforço.
:::

------------------------------------------------------------------------

## Aprendizado supervisionado

::: column-screen-inset
<iframe src="imgs/supervisionado.html" width="2000px" height="600px">

</iframe>
:::

::: notes
No aprendizado supervisionado nós temos ainda a regressão e a classificação. Na regressão o objetivo é prever um valor contínuo e na classificação separar as classes como no exemplo que eu já citei.
:::

## Regressão {auto-animate="true"}

```{python}
import numpy as np
import pandas as pd
import plotly.express as px

# Parâmetros
np.random.seed(42)
meses = pd.date_range(start='2022-01-01', end='2024-12-01', freq='MS')
n = len(meses)

# Tendência de crescimento linear
tendencia = np.linspace(10000, 40000, n)

# Sazonalidade anual (picos em dezembro)
sazonalidade = 5000 * np.sin(2 * np.pi * (meses.month - 1) / 12)

# Ruído aleatório
ruido = np.random.normal(0, 1500, n)

# Receita simulada
receita = tendencia + sazonalidade + ruido

# DataFrame
df = pd.DataFrame({
    'Mês': meses,
    'Receita Mensal (R$)': receita
})

# Gráfico
fig = px.scatter(df, x='Mês', y='Receita Mensal (R$)', 
                 title='Receita Mensal de uma Loja Online (2022-2024)',
                 labels={'Receita Mensal (R$)': 'Receita Mensal (R$)'},
                 )

fig.update_traces(marker=dict(size=8, color='royalblue'))
fig.show()


```

::: notes
E aí a regressão é um ótimo ponto de partida pra que nós possamos começar a entender como funciona a generalização de um método de machine learning. Vamos pegar esse exemplo onde queremos em um dia x intermediário entre dezembro e janeiro de 2023.
:::

## Regressão {auto-animate="true"}

```{python}
import numpy as np
import pandas as pd
import plotly.express as px

# Parâmetros
np.random.seed(42)
meses = pd.date_range(start='2022-01-01', end='2024-12-01', freq='MS')
n = len(meses)

# Tendência de crescimento linear
tendencia = np.linspace(10000, 40000, n)

# Sazonalidade anual (picos em dezembro)
sazonalidade = 5000 * np.sin(2 * np.pi * (meses.month - 1) / 12)

# Ruído aleatório
ruido = np.random.normal(0, 1500, n)

# Receita simulada
receita = tendencia + sazonalidade + ruido

# DataFrame
df = pd.DataFrame({
    'Mês': meses,
    'Receita Mensal (R$)': receita
})

# Gráfico
fig = px.scatter(df, x='Mês', y='Receita Mensal (R$)', 
                 title='Receita Mensal de uma Loja Online (2022-2024)',
                 labels={'Receita Mensal (R$)': 'Receita Mensal (R$)'},
                 trendline="ols")

fig.update_traces(marker=dict(size=8, color='royalblue'))
for trace in fig.data:
    if trace.mode == 'lines':
        trace.line.color = 'red'
fig.show()


```

::: notes
Pra fazer isso eu posso muito bem ajustar uma reta a esses dados.
:::

## Regressão

::: fragment
$$
y = a \cdot x +b 
$$
:::

::: fragment
$$
y = 0.00028 \cdot x -452196 
$$
:::

::: notes
Quando eu ajusto uma reta aos dados eu estou calculando os coeficientes dessa reta. De forma que tendo os coeficientes eu consigo fazer uma estimativa dos valores intermediários ou até uma tentativa de extrapolação fazendo uma projeção da reta para o futuro. Já que nesse caso nos estamos com um caso particular de regressão que envolve uma série temporal. Ou seja, ajustando a reta e determinando os coeficientes estamos conseguindo generalizar o problema e calcular o valor da receita mesmo em datas onde não coletamos o valor da receita.
:::

## Regressão

```{python}
import numpy as np
import pandas as pd
import plotly.graph_objects as go

# Parâmetros
np.random.seed(42)
meses = pd.date_range(start='2022-01-01', end='2024-12-01', freq='MS')
n = len(meses)

# Tendência de crescimento linear
tendencia = np.linspace(10000, 40000, n)

# Sazonalidade anual (picos em dezembro)
sazonalidade = 5000 * np.sin(2 * np.pi * (meses.month - 1) / 12)

# Ruído aleatório
ruido = np.random.normal(0, 1500, n)

# Receita simulada
receita = tendencia + sazonalidade + ruido

# DataFrame
df = pd.DataFrame({
    'Mês': meses,
    'Receita Mensal (R$)': receita
})

# Transformando a data em número para ajuste polinomial
x = np.arange(n)  # pode ser dias ou meses, aqui estamos usando a posição do mês
y = receita

# Ajuste polinomial de grau 4 (você pode mudar para 3, 5, etc)
grau = 9
coef = np.polyfit(x, y, grau)
polinomio = np.poly1d(coef)

# Valores ajustados pelo polinômio
y_fit = polinomio(x)

# Gráfico
fig = go.Figure()

# Pontos reais
fig.add_trace(go.Scatter(
    x=df['Mês'], y=y, mode='markers',
    marker=dict(size=8, color='royalblue'),
    name='Receita Mensal'
))

# Curva polinomial ajustada
fig.add_trace(go.Scatter(
    x=df['Mês'], y=y_fit, mode='lines',
    line=dict(color='red', width=3),
    name=f'Ajuste Polinomial (grau {grau})'
))

fig.update_layout(
    title='Receita Mensal de uma Loja Online (2022-2024) com Ajuste Polinomial',
    xaxis_title='Mês',
    yaxis_title='Receita Mensal (R$)'
)

fig.show()

```

::: notes
É claro que podemos ver que a reta talvez não seja a melhor solução para esse caso. Poderia ajustar um polinômio talvez.
:::

## Classificação

```{python}
import numpy as np
import pandas as pd
import plotly.express as px

np.random.seed(42)
n = 100
desconto = np.random.uniform(0, 50, n)

def sigmoid(x):
    return 1 / (1 + np.exp(-(x-25)/5))

prob_compra = sigmoid(desconto)
classe = np.random.binomial(1, prob_compra)

df = pd.DataFrame({'Desconto (%)': desconto, 'Comprou': classe})

# Converta a coluna para string para garantir que seja categórica
df['Comprou'] = df['Comprou'].astype(str)

fig = px.scatter(
    df, x='Desconto (%)', y='Comprou',
    color='Comprou',
    color_discrete_map={'0': 'red', '1': 'blue'},
    category_orders={'Comprou': ['1', '0']},  # Ordem correta: 0 embaixo, 1 em cima
    title='Probabilidade de Compra em Função do Desconto',
    labels={'Comprou': 'Classe (0=Não comprou, 1=Comprou)'}
)

fig.update_traces(marker=dict(size=10, opacity=0.7, line=dict(width=1, color='black')))

fig.show()


```

::: notes
Já a classificação em um primeiro momento é um pouco mais difícil de pensar como um ajuste de curva aos dados porque estamos tentando separar duas classes diferentes.
:::

## Classificação

```{python}
import numpy as np
import pandas as pd
import plotly.express as px

np.random.seed(42)
n = 100
desconto = np.random.uniform(0, 50, n)

def sigmoid(x):
    return 1 / (1 + np.exp(-(x-25)/5))

prob_compra = sigmoid(desconto)
classe = np.random.binomial(1, prob_compra)

df = pd.DataFrame({'Desconto (%)': desconto, 'Comprou': classe})

fig = px.scatter(
    df, x='Desconto (%)', y='Comprou',
    color=df['Comprou'].astype(str),
    color_discrete_map={'0': 'red', '1': 'blue'},
    title='Probabilidade de Compra em Função do Desconto',
    labels={'Comprou': 'Classe (0=Não comprou, 1=Comprou)'}
)
fig.update_traces(marker=dict(size=10, opacity=0.7, line=dict(width=1, color='black')))

# Adicionando a curva sigmoide
x_curve = np.linspace(0, 50, 200)
y_curve = sigmoid(x_curve)

fig.add_scatter(
    x=x_curve,
    y=y_curve,
    mode='lines',
    name='Curva Sigmoide',
    line=dict(color='black', width=3)
)

fig.show()



```

::: notes
Mas também podemos ver o problema dessa forma. Nesse caso por exemplo podemos ajustar uma curva do tipo sigmóide.
:::

## Classificação

```{python}
import numpy as np
import pandas as pd
import plotly.express as px

np.random.seed(42)
n = 100
desconto = np.random.uniform(0, 50, n)

def sigmoid(x):
    return 1 / (1 + np.exp(-(x-25)/5))

prob_compra = sigmoid(desconto)
classe = np.random.binomial(1, prob_compra)

df = pd.DataFrame({'Desconto (%)': desconto, 'Comprou': classe})

fig = px.scatter(
    df, x='Desconto (%)', y='Comprou',
    color=df['Comprou'].astype(str),
    color_discrete_map={'0': 'red', '1': 'blue'},
    title='Probabilidade de Compra em Função do Desconto',
    labels={'Comprou': 'Classe (0=Não comprou, 1=Comprou)'}
)
fig.update_traces(marker=dict(size=10, opacity=0.7, line=dict(width=1, color='black')))

# Adicionando a curva sigmoide
x_curve = np.linspace(0, 50, 200)
y_curve = sigmoid(x_curve)

fig.add_scatter(
    x=x_curve,
    y=y_curve,
    mode='lines',
    name='Curva Sigmoide',
    line=dict(color='black', width=3)
)

# Adicionando a linha divisória em y=0.5
fig.add_shape(
    type="line",
    x0=0, x1=50,
    y0=0.5, y1=0.5,
    line=dict(color="green", width=2, dash="dash"),
    name='Divisão 0.5'
)

# Opcional: adicionar anotação indicando o limiar
fig.add_annotation(
    x=2, y=0.52,
    text="Limiar 0.5",
    showarrow=False,
    font=dict(color="green", size=12),
    bgcolor="white"
)

fig.show()

```

::: notes
e depois definir em que nível de diferença dos valores da sigmóide vamos separar os dados.
:::

## Classificação

$$ 
\sigma(\mathbf{x}) = \frac{1}{1 + e^{-(w^T \mathbf{x} + b)}}
$$

## Aprendizado supervisionado

[Scikit-learn](https://scikit-learn.org/stable/supervised_learning.html)

::: notes
É claro que esses são apenas alguns dos métodos disponíveis para regressão e classificação se entrarmos por exemplo na página da sklearn vamos ver inúmeros outros métodos que vão ser mais ou menos eficientes para se ajustar a tipos de dados diferentes de n tipos de problemas de regressão e classificação.
:::

## Aprendizado supervisionado

| Id  | Idade | IMC  | Glicose | Pressão | Diabetes 🎯 |
|-----|-------|------|---------|---------|-------------|
| 1   | 45    | 29.0 | 130     | 80      | ✅ Sim      |
| 2   | 34    | 22.5 | 98      | 70      | ❌ Não      |
| 3   | 54    | 31.2 | 145     | 85      | ✅ Sim      |
| 4   | 29    | 24.1 | 92      | 75      | ❌ Não      |
| 5   | 62    | 33.8 | 160     | 90      | ✅ Sim      |

::: notes
Ainda nos problemas de apredizagem supervisionado, a característica mais importante é que nesses problemas eu tenho um alvo. Por exemplo nesse caso eu tenho a classe se a pessoa tem ou não diabetes, ou no problema financeiro de regressão eu tenho os valores monetários para cada data. O que é importante notar é esses valores são valores que aprenda de alguma forma a descobrir quando passo por exemplo as características do paciente ou do problema e pra que ele consiga aprender sobre o problema nós precisamos ter esses valores registrados. Alguém precisou ir lá entrevistar ou avaliar o caso de n pacientes e anotar os pacientes com as características x tem diabetes e os com as características y não, depois os z tem...
:::

## Detecção de fraude

![Robô cometendo fraude](imgs/fraude.png)

::: notes
Embora os dados tabulares sejam o exemplo mais simples de dados que nós temos existem muitas aplicações sobre esse tipo de dados. Um exemplo é a detecção de fraude. Modelos supervisionados (como Random Forest, XGBoost) treinados com histórico de transações rotuladas como “fraudulentas” ou “legítimas”.
:::

## Previsão de churn

``` markdown
| customer_id | idade | tempo_de_contrato (meses) | valor_mensal | utilizacao_app (dias/mês) | reclamacoes | churn |
|-------------|-------|--------------------------|--------------|---------------------------|-------------|-------|
| 001         | 25    | 12                       | 79.90        | 22                        | 0           | 0     |
| 002         | 42    | 6                        | 99.90        | 10                        | 2           | 1     |
| 003         | 34    | 24                       | 59.90        | 25                        | 0           | 0     |
| 004         | 28    | 3                        | 89.90        | 5                         | 1           | 1     |
| 005         | 50    | 36                       | 49.90        | 30                        | 0           | 0     |
```

::: notes
Empresas de SaaS, telecom, bancos e varejo querem prever quais clientes estão propensos a cancelar seus serviços.
:::

## Otimização de cadeia de suprimentos

``` markdown
| produto_id | centro_distribuicao | estoque_atual | demanda_prevista | tempo_reposicao (dias) | custo_transporte | prioridade |
|------------|---------------------|---------------|------------------|-----------------------|------------------|------------|
| 1001       | SP                  | 150           | 200              | 2                     | 500              | 1          |
| 1002       | RJ                  | 80            | 60               | 3                     | 400              | 2          |
| 1003       | MG                  | 50            | 100              | 5                     | 700              | 1          |
| 1004       | RS                  | 200           | 180              | 4                     | 300              | 3          |
| 1005       | BA                  | 90            | 120              | 6                     | 600              | 2          |
```

::: notes
Previsão de demanda para estoques, logística e produção.
:::

## Aprendizado não-supervisionado

| Id  | Idade | IMC  | Glicose | Pressão |
|-----|-------|------|---------|---------|
| 1   | 45    | 29.0 | 130     | 80      |
| 2   | 34    | 22.5 | 98      | 70      |
| 3   | 54    | 31.2 | 145     | 85      |
| 4   | 29    | 24.1 | 92      | 75      |
| 5   | 62    | 33.8 | 160     | 90      |

::: notes
Mas eventualmente nós nos deparamos com problemas que é a primeira vez que nós estamos vendo. Nós nem sequer sabemos que a diabetes existe, por exemplo. Mas aí nos nossos registros médicos vamos anotando os dados das pessoas e vamos percebendo algumas características.
:::

## Clustering

```{python}

import pandas as pd
import plotly.express as px
from sklearn.datasets import make_blobs

# Gerar dados sintéticos com 2 clusters
X, y = make_blobs(n_samples=100, centers=2, cluster_std=2.5, random_state=42)

# Ajustar os valores para simular IMC e Glicose em faixas realistas
# IMC: entre 18 e 40, Glicose: entre 70 e 200
IMC = 18 + (X[:, 0] - X[:, 0].min()) * (40 - 18) / (X[:, 0].max() - X[:, 0].min())
Glicose = 70 + (X[:, 1] - X[:, 1].min()) * (200 - 70) / (X[:, 1].max() - X[:, 1].min())

# Criar DataFrame
df = pd.DataFrame({
    'IMC': IMC,
    'Glicose': Glicose,
    'Grupo': ['Grupo 1' if label == 0 else 'Grupo 2' for label in y]
})

# Plot com Plotly
fig = px.scatter(
    df, x='IMC', y='Glicose',
    color='Grupo',
    title='Clustering Simulado: IMC vs Glicose',
    labels={'IMC': 'IMC', 'Glicose': 'Glicose'}
)
fig.show()



```

::: notes
E aí um primeiro tipo de algoritmo bastanta conhecido seria um algoritmo de clustering e esse algoritmo ajudaria a gente a tentar discernir algo em cima dos dados. Eventualmente ajudando na análise exploratória e indicando de que forma poderíamos anotar esses dados trasnformando o problema em um problema supervisionado em algum momento.
:::

## PCA

```{python}
import pandas as pd
import plotly.express as px
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA

# Carregar um conjunto de dados de exemplo (Iris)
data = load_iris()
X = data.data
y = data.target
target_names = data.target_names

# Aplicar PCA para reduzir para 2 dimensões
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Criar DataFrame para plotagem
df = pd.DataFrame({
    'PC1': X_pca[:, 0],
    'PC2': X_pca[:, 1],
    'Classe': [target_names[i] for i in y]
})

# Plot com Plotly
fig = px.scatter(
    df, x='PC1', y='PC2',
    color='Classe',
    title='PCA: Projeção dos Dados nos Dois Primeiros Componentes',
    labels={'PC1': 'Componente Principal 1', 'PC2': 'Componente Principal 2'}
)
fig.show()

```

::: notes
O **PCA (Análise de Componentes Principais)** é um algoritmo de aprendizado não-supervisionado usado para reduzir a dimensionalidade dos dados, transformando variáveis originais em novas variáveis (componentes principais) que capturam a maior parte da variância presente nos dados, facilitando visualização e análise de padrões.
:::

## Aplicações

-   Segmentação de clientes

-   Detecção de anomalias

-   Agrupamento de textos

-   Descoberta de padrões de consumo (regras de associação)

## Aprendizagem por reforço

![Genshin Impact](https://mygear.vn/media/lib/05-12-2022/ge4.jpg)

::: notes
Aprendizagem por reforço é uma técnica em que um agente aprende a tomar decisões sequenciais ao interagir com um ambiente, recebendo recompensas ou punições para maximizar seu desempenho ao longo do tempo.
:::

## Aprendizagem por reforço

![Aprendizagem por reforço](imgs/reforco.png)

::: notes
Aprendizagem por reforço é uma técnica em que um agente aprende a tomar decisões sequenciais ao interagir com um ambiente, recebendo recompensas ou punições para maximizar seu desempenho ao longo do tempo.
:::

## Perceptron

::: column-screen-inset
<iframe src="imgs/perceptron.html" width="2000px" height="600px">

</iframe>
:::

## Perceptron
$$
\hat{y} = f\left( \sum_{i=0}^{m} w_i x_i \right)
$$

## Redes Neurais Profundas

::: column-screen-inset
<iframe src="imgs/profunda.html" width="2000px" height="600px">

</iframe>
:::

## Treinando uma rede

::: column-screen-inset
<iframe src="imgs/rede_neural.html" width="2000px" height="600px">

</iframe>
:::


## Propagação 


| Id  | Idade | IMC  | Glicose | Pressão | Diabetes 🎯 |
|-----|-------|------|---------|---------|-------------|
| 1   | 45    | 29.0 | 130     | 80      | ✅ Sim      |
| 2   | 34    | 22.5 | 98      | 70      | ❌ Não      |
| 3   | 54    | 31.2 | 145     | 85      | ✅ Sim      |
| 4   | 29    | 24.1 | 92      | 75      | ❌ Não      |
| 5   | 62    | 33.8 | 160     | 90      | ✅ Sim      |

## Retropropagação

::: column-screen-inset
<iframe src="imgs/retro.html" width="2000px" height="600px">

</iframe>
:::

## Arquiteturas de Redes Neurais

![https://www.asimovinstitute.org/neural-network-zoo/](https://www.asimovinstitute.org/wp-content/uploads/2019/04/NeuralNetworkZo19High.png)

## Redes Neurais Convolucionais (CNNs)

![](imgs/oia.png)

## Redes Neurais Convolucionais (CNNs)

![](imgs/oia_vermelho.png)

## Redes Neurais Convolucionais (CNNs)

![https://alexlenail.me/NN-SVG/LeNet.html](imgs/convolucional.png)

## Visão Computacional para Indústria 4.0

![https://www.kaggle.com/datasets/salmaneunus/railway-track-fault-detection](imgs/trilho.jpg)

## Saúde

![https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia](imgs/medical.jpeg)

## Sustentabilidade e Meio Ambiente

![https://www.kaggle.com/datasets/akhilchibber/deforestation-detection-dataset](imgs/floresta.jpg)

## Detecção de objetos - Agricultura

![https://www.kaggle.com/datasets/trainingdatapro/ripe-strawberries-detection](imgs/morangos.png)

## Segmentação de objetos

![https://www.sciencedirect.com/science/article/abs/pii/S0926985117307632](imgs/salt.jpg)

## Transferência de aprendizado

![https://www.cs.toronto.edu/~kriz/cifar.html](imgs/cifar10.png)

## Transferência de aprendizado

- Fine-tuning

::: notes

Como funciona: Um modelo pré-treinado em uma tarefa grande (ex: ImageNet para imagens, ou Wikipedia para texto) tem suas camadas finais ajustadas (ou toda a rede é re-treinada com uma taxa de aprendizado menor) para uma nova tarefa específica.
Exemplo: Pegar um ResNet treinado para classificar 1000 objetos e ajustar para detectar tipos de defeitos em peças industriais.

:::

## Transferência de aprendizado

- Fine-tuning

- Transferência de domínio 

::: notes

Como funciona: O modelo é treinado em um domínio (ex: fotos de câmeras profissionais) e depois adaptado para outro (ex: imagens de celulares), geralmente usando técnicas para reduzir a diferença entre os domínios.
Exemplo: Treinar um modelo de reconhecimento facial em fotos de estúdio e adaptá-lo para funcionar em selfies.

:::

## Redes Neurais Recorrentes (RNNs)

::: column-screen-inset
<iframe src="imgs/rnn.html" width="2000px" height="600px">

</iframe>
:::

## Redes Neurais Recorrentes (RNNs)

 - LSTM (Long Short-Term Memory)

 - GRU ( Gated Recurrent Unit )

## Séries temporais

![https://www.kaggle.com/datasets/abhisheksjha/time-series-air-quality-data-of-india-2010-2023/data](imgs/timeseries.png)

## IoT e sensores

![](imgs/watch.jpg)

## Completação de texto ou código

Time Series Air **Quality**

## Recursos

- [HuggingFace](https://huggingface.co)

- [Kaggle](https://www.kaggle.com)

- [PapersWithCode](https://paperswithcode.com)

## FrameWorks

- [TensorFlow](https://www.tensorflow.org)

- [Pytorch](https://pytorch.org)

- [HuggingFace](https://huggingface.co/docs)

- [Jax](https://docs.jax.dev/en/latest/quickstart.html)

## Inteligência Artificial Generativa

![IA Generativa](imgs/generativeai.png)


::: notes
Chegamos então à Inteligência Artificial Generativa (Generative AI). Esta é uma subárea da IA focada na criação de modelos capazes de gerar conteúdo novo e original que se assemelha aos dados com os quais foram treinados. Em vez de apenas classificar ou prever (IA discriminativa), a IA generativa cria. Isso pode incluir texto, imagens, música, código, vídeos, designs, estruturas moleculares, e mais. Ela aprende os padrões e a estrutura subjacente dos dados de treinamento e usa esse conhecimento para produzir novas amostras.
:::

## O que é Generative AI?

 - Objetivo: Aprender a distribuição de probabilidade dos dados de treinamento $P(\mathbf{x})$.
 - Tarefa: Gerar novas amostras $\mathbf{x}_{novo} \sim P(\mathbf{x})$ que sejam realistas e diversas.


## O que é Generative AI?

 - Diferença para IA Discriminativa:
    - Discriminativa (ex: Classificação): Aprende $P(y|\mathbf{x})$ - a probabilidade da classe $y$ dado a entrada $\mathbf{x}$. Foco em mapear entrada para saída.
    - Generativa: Aprende $P(\mathbf{x})$ (ou $P(\mathbf{x}, y)$). Foco em entender como os dados são gerados.
Tecnologias Chave: LLMs (Transformers), Modelos de Difusão, GANs, VAEs.

::: notes
Formalmente, o objetivo da IA Generativa é aprender a distribuição de probabilidade dos dados de treinamento. Uma vez que o modelo aprendeu essa distribuição, ele pode "amostrar" dela para gerar novos dados que pareçam vir da mesma distribuição. Isso contrasta com modelos discriminativos, que aprendem a separar diferentes tipos de dados (ex: classificar emails como spam ou não spam) aprendendo a fronteira de decisão entre eles, sem necessariamente saber como gerar um email de spam ou não spam. As tecnologias mais proeminentes hoje para IA Generativa incluem LLMs (baseados em Transformers), Modelos de Difusão e GANs.
:::

## Transformers

![Transformer](imgs/transformer.png)

::: notes
A arquitetura Transformer, introduzida no paper "Attention Is All You Need" (2017), revolucionou o processamento de linguagem natural e se expandiu para outras áreas. Sua principal inovação é o mecanismo de auto-atenção (self-attention). Diferente das RNNs que processam sequências passo a passo (o que limita a paralelização e dificulta capturar dependências longas), a atenção permite que o modelo pondere a importância de todas as outras palavras na sequência ao processar uma palavra específica, independentemente da distância. Isso permite capturar relações de longo alcance de forma eficiente e processar a sequência em paralelo, levando a treinamentos mais rápidos e modelos mais poderosos.
:::

## Completação

Hoje acordei e

## Completação

\[Hoje\] \[acordei\] \[e\]

## Completação

\[Hoje\] \[acordei\] \[e\]

\[caí \] .9

\[ \] 0.01

\[ levantei \] 0.008


## Completação

\[Hoje\] \[acordei\] \[e\] \[caí \]

## Completação

\[Hoje\] \[acordei\] \[e\] \[caí \]

\[da \]  0.8

\[pizza \] 0.1

\[ gato \] 0.1

## Tokenização

"O mundo é redondo, mas está ficando cada mais quadrado."

## Byte Pair Encoding - Passo 1

Primeiro, dividimos o texto em palavras e adicionamos um símbolo especial ao final de cada palavra (por exemplo, </w> para indicar o fim da palavra):

"O</w> mundo</w> é</w> redondo,</w> mas</w> está</w> ficando</w> cada</w> mais</w> quadrado.</w>
"

## Byte Pair Encoding - Passo 2

Cada palavra é decomposta em caracteres (incluindo o símbolo de fim de palavra):

O </w>

m u n d o </w>

é </w>

r e d o n d o , </w>

m a s </w>

e s t á </w>

f i c a n d o </w>

c a d a </w>

m a i s </w>

q u a d r a d o . </w>


## Byte Pair Encoding - Passo 3

Agora, contamos todos os pares de caracteres adjacentes mais frequentes em todas as palavras. Exemplo: "m u", "u n", "n d", "d o" em "mundo".

## Byte Pair Encoding - Passo 4

Unimos o par mais frequente em um novo símbolo. Por exemplo, se "a s" aparece muito, vira "as". Repetimos esse processo várias vezes.

Exemplo de iterações (simplificado):

Iteração 1: "a s" → "as"

Iteração 2: "m a" → "ma"

Iteração 3: "n d" → "nd"

Iteração 4: "d o" → "do"
...


## Byte Pair Encoding - Passo 5

Depois de várias junções, as palavras originais são representadas como sequências de subpalavras ou tokens BPE.

Exemplo (imaginando algumas junções):

"quadrado" pode virar: ["qua", "dra", "do", ". </w>"]

"ficando" pode virar: ["fi", "can", "do", "</w>"]

"redondo" pode virar: ["re", "don", "do", ", </w>"]

## Raízes das palavras

feliz → ["feliz"]

felicidade → ["feli", "cidade"]

infeliz → ["in", "feliz"]

infelicidade → ["in", "feli", "cidade"]

felizmente → ["feliz", "mente"]

infelizmente → ["in", "feliz", "mente"]

superfeliz → ["super", "feliz"]

## Embeddings

| O   | mun | do | é | re | don | do , | mas | es | tá | fi | can | do | ca | da | vez | mais | qua | dra | do . | ... |
|-----|-----|----|---|----|-----|------|-----|----|----|----|-----|----|----|----|-----|------|-----|-----|------|-----|
| 0.3 | 0.2 | 0.4|0.2|0.6 | 0.5 | 0.3  | 0.4 |0.2 |0.5 |0.7 |0.3  |0.2 |0.4 |0.5 |0.6  | 0.9  |0.5  |0.4  |0.3   | ... |
| 0.7 | 0.8 | 0.1|0.9|0.3 | 0.7 | 0.2  | 0.8 |0.7 |0.2 |0.1 |0.8  |0.5 |0.3 |0.1 |0.2  | 0.1  |0.4  |0.2  |0.7   | ... |
| 0.2 | 0.4 | 0.3|0.7|0.7 | 0.6 | 0.8  | 0.6 |0.9 |0.4 |0.4 |0.2  |0.8 |0.7 |0.6 |0.9  | 0.4  |0.2  |0.5  |0.8   | ... |
| 0.5 | 0.1 | 0.7|0.1|0.2 | 0.3 | 0.4  | 0.2 |0.6 |0.1 |0.6 |0.5  |0.1 |0.8 |0.7 |0.1  | 0.8  |0.7  |0.3  |0.2   | ... |




## Embeddings

- Cada token é representado por um vetor

[Projector TensorFlow](https://projector.tensorflow.org)

::: notes
Embeddings são representações vetoriais densas e de baixa dimensão de itens discretos, como palavras, usuários, produtos ou até mesmo nós em um grafo. Em vez de representar uma palavra com um vetor esparso enorme (one-hot encoding), usamos um vetor denso de números reais (ex: 300 dimensões). Esses vetores são aprendidos (geralmente por redes neurais como Word2Vec, GloVe, ou como parte de modelos maiores como Transformers) de forma que itens semanticamente similares tenham vetores próximos no espaço vetorial. Por exemplo, os vetores para "rei" e "rainha" estarão próximos, e a relação entre "rei" e "rainha" pode ser similar à relação entre "homem" e "mulher" (capturada por operações vetoriais). Embeddings são fundamentais para que modelos de ML/DL processem dados categóricos e textuais de forma eficaz. O TensorFlow Embedding Projector é uma ferramenta fantástica para visualizar esses espaços vetoriais.
:::

## Tradução

![https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png)

## Detalhando

![https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png)

## Encoder

![https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/images/t/Transformer_encoder.png)

## Arquitetura transformers

![Arquitetura](imgs/diagrama-7.webp)

## Aplicações de Transformers

- Processamento de Linguagem Natural (NLP):
    - Tradução Automática (Google Translate)
    - Geração de Texto (GPT, Gemini, Llama, Claude - base das LLMs)
    - Sumarização de Texto
    - Respondendo Perguntas (Question Answering)
    - Análise de Sentimento
    - Classificação de Texto

## Aplicações de Transformers

- Visão Computacional:
    - Classificação de Imagens (Vision Transformer - ViT)
    - Detecção de Objetos (DETR)
    - Segmentação de Imagens
    - Geração de Imagens (a partir de texto, combinado com outras técnicas)

## Aplicações de Transformers

- Outras Áreas:
    - Previsão de Séries Temporais
    - Bioinformática (AlphaFold - previsão de estrutura de proteínas)
    - Sistemas de Recomendação
    - Processamento de Áudio


## LLMs (Large Language Models)

- Definição: Modelos de linguagem baseados em Transformers com bilhões (ou trilhões) de parâmetros, treinados em quantidades massivas de texto (internet, livros, etc.).

## LLMs (Large Language Models)


- Capacidades:
    - Compreensão e geração de texto coerente e contextualmente relevante.
    - Tradução, sumarização, resposta a perguntas.
    - Raciocínio (limitado), escrita criativa, geração de código.
    - Few-shot / Zero-shot learning: Capacidade de realizar tarefas para as quais não foram explicitamente treinados, apenas com base na descrição da tarefa ou poucos exemplos no prompt.

## LLMs (Large Language Models)

- Exemplos: GPT-4 (OpenAI), Gemini (Google), Llama 3 (Meta), Claude 3 (Anthropic).

::: notes
Os Large Language Models (LLMs) são talvez o exemplo mais conhecido de IA Generativa atualmente. São essencialmente modelos Transformer escalados para tamanhos enormes em termos de número de parâmetros e dados de treinamento. Esse escalonamento massivo levou a habilidades emergentes notáveis, como a capacidade de seguir instruções complexas, gerar texto muito fluente e realizar tarefas com pouca ou nenhuma exemplificação prévia (few-shot/zero-shot learning). Eles são a base de chatbots como ChatGPT, Gemini e Claude.
:::

## GANs (Redes Generativas Adversariais)

![](imgs/gan_faces.png)

## GANs (Redes Generativas Adversariais)

![](imgs/gan.png)

## GANs (Redes Generativas Adversariais)

 - Conceito: Duas redes neurais competindo:
    - Gerador (Generator): Tenta criar dados sintéticos (ex: imagens falsas) que pareçam reais. Começa a partir de um vetor de ruído aleatório.
    - Discriminador (Discriminator): Tenta distinguir entre dados reais (do dataset de treino) e dados falsos (criados pelo Gerador). É um classificador binário.

## GANs (Redes Generativas Adversariais)

- Treinamento Adversarial:
    - O Gerador aprende a enganar o Discriminador.
    - O Discriminador aprende a não ser enganado.
    - Elas melhoram juntas em um jogo de "gato e rato".

## GANs (Redes Generativas Adversariais)

- Aplicações: Geração de imagens realistas, style transfer, super-resolução, data augmentation.
- Desafios: Treinamento pode ser instável (mode collapse, non-convergence).

::: notes
As Redes Generativas Adversariais (Generative Adversarial Networks - GANs) foram uma das primeiras técnicas a gerar imagens realistas. A ideia é colocar duas redes em competição. O Gerador cria amostras falsas, e o Discriminador tenta identificar se uma amostra é real ou falsa. O Gerador é treinado para maximizar a probabilidade de o Discriminador classificar suas saídas como reais, enquanto o Discriminador é treinado para classificar corretamente. Esse processo adversarial força o Gerador a produzir amostras cada vez mais indistinguíveis das reais. Embora poderosas, as GANs podem ser difíceis de treinar.
:::

## VAEs

![VAE](imgs/vae.png)

## Modelos de Difusão

![U-NET](imgs/unet.png)

## Modelos de Difusão

- Conceito: Geram dados (principalmente imagens) aprendendo a reverter um processo de adição de ruído.

## Modelos de Difusão


- Processo:
    - Forward Process (Noising): Gradualmente adiciona ruído Gaussiano a uma imagem real ao longo de vários passos, até que se torne puro ruído.
    - Reverse Process (Denoising): Treina uma rede neural (geralmente uma U-Net) para prever e remover o ruído adicionado em cada passo, começando de um ruído aleatório e gradualmente reconstruindo uma imagem coerente.

## Modelos de Difusão

- Qualidade: Produzem imagens de altíssima qualidade e realismo.
- Controle: Podem ser condicionados por texto (text-to-image) ou outras imagens.
- Exemplos: DALL-E 2/3, Stable Diffusion, Midjourney, Imagen.

::: notes
Os Modelos de Difusão se tornaram a técnica dominante para geração de imagens de alta qualidade. A ideia central é engenhosa: primeiro, eles aprendem como destruir sistematicamente a informação em uma imagem adicionando ruído (processo forward, que é fácil de definir). Depois, eles treinam uma rede neural para fazer o processo inverso: começar com puro ruído e, passo a passo, remover o ruído de forma inteligente para gerar uma imagem realista (processo reverse, que é difícil e requer aprendizado). Ao condicionar o processo de denoising a um prompt de texto, obtemos os poderosos modelos text-to-image.
:::



## Diferenças entre LLMs, GANs e Modelos de Difusão

```Markdown
| Característica        | LLMs (Transformers)                   | Modelos de Difusão                            | GANs                                         |
|-----------------------|----------------------------------------|------------------------------------------------|----------------------------------------------|
| Domínio Principal     | Texto, Código                          | Imagens (alta qualidade), Áudio                | Imagens, Vídeo, Dados Tabulares              |
| Mecanismo Base        | Auto-atenção, Predição Autoregressiva  | Reversão de processo de ruído (Denoising)     | Competição Gerador vs Discriminador          |
| Qualidade (Imagem)    | N/A (para texto é alta)                | Muito Alta                                     | Alta (mas pode ter artefatos)                |
| Estabilidade Treino   | Geralmente estável (custoso)           | Geralmente estável                             | Pode ser instável (mode collapse)            |
| Controle Geração      | Prompt de texto                        | Prompt de texto, Imagem                        | Vetor Latente, Condicionamento               |
| Velocidade Geração    | Rápida (paralelizável)                 | Lenta (múltiplos passos de denoising)          | Rápida (um passo do gerador)                 |
```

::: notes

Embora todos sejam modelos generativos, existem diferenças importantes:

LLMs são baseados em Transformers e dominam tarefas de texto devido à sua capacidade de lidar com sequências longas e atenção.
Modelos de Difusão se destacam na geração de imagens de alta fidelidade, através de um processo iterativo de denoising, mas podem ser lentos para gerar.
GANs usam um treinamento adversarial único, são rápidas na geração (após treinadas), mas podem ser instáveis para treinar e às vezes sofrem de "mode collapse" (geram pouca variedade). A escolha entre eles depende da tarefa, do tipo de dados e dos requisitos de qualidade e velocidade. 

:::



## Desafios e Limitações da IA

Apesar dos avanços impressionantes, a IA (incluindo ML, DL e Generativa) ainda enfrenta desafios significativos:

- Complexidade e Interpretabilidade ("Black Box")
- Viés e Justiça
- Privacidade e Segurança
- Robustez e Generalização
 - Ética e Impacto Social

::: notes
É crucial reconhecer que a IA não é uma solução mágica e possui limitações e desafios importantes. Ignorar esses desafios pode levar a falhas, injustiças e consequências negativas. Vamos abordar alguns dos principais.
:::


## IA Explicável (XAI)

- Problema: Modelos complexos (Deep Learning, LLMs) funcionam como "caixas-pretas" - é difícil entender por que eles tomam uma decisão específica.
- Objetivo (XAI): Desenvolver técnicas para tornar as previsões e decisões dos modelos de IA compreensíveis para humanos.

## IA Explicável (XAI)

- Importância:
    - Confiança: Usuários precisam confiar nas decisões (ex: diagnóstico médico).
    - Debugging: Entender por que um modelo falha.
    - Justiça (Fairness): Verificar se o modelo não usa vieses indesejados.
    - Conformidade (Compliance): Regulamentações (como GDPR) podem exigir explicabilidade.
- Técnicas: SHAP, LIME, Mapas de Atenção, Contrafactuais.

::: notes
Um grande desafio, especialmente com redes neurais profundas e LLMs, é a falta de transparência. Por que o modelo recomendou este produto? Por que ele negou o crédito? Por que ele gerou essa resposta específica? A IA Explicável (XAI) busca responder a essas perguntas, fornecendo insights sobre o funcionamento interno dos modelos. Isso é vital para construir confiança, depurar erros, garantir a justiça e cumprir regulamentações.
:::

## Viés Algorítmico

- Problema: Modelos de IA aprendem a partir de dados, e se os dados refletem vieses históricos ou sociais (de gênero, raça, etc.), o modelo aprenderá e poderá até amplificar esses vieses.

## Viés Algorítmico


- Consequências: Resultados injustos ou discriminatórios em áreas críticas como contratação, concessão de crédito, reconhecimento facial, justiça criminal.
- Fontes de Viés: Dados de treinamento desbalanceados ou tendenciosos, escolhas de features, design do algoritmo.

## Viés Algorítmico

- Mitigação:
    - Curadoria cuidadosa e auditoria dos dados.
    - Uso de métricas de justiça (fairness metrics).
    - Algoritmos "conscientes da justiça" (fairness-aware algorithms).
    - Pós-processamento das saídas do modelo.

::: notes
O viés algorítmico é uma preocupação séria. Os modelos não são inerentemente neutros; eles refletem o mundo (e os dados) com os quais foram treinados. Se esses dados contêm preconceitos, o modelo os aprenderá. Isso pode levar a sistemas que perpetuam ou até pioram as desigualdades existentes. Combater o viés exige um esforço consciente em todas as etapas do desenvolvimento do modelo, desde a coleta de dados até a avaliação e o monitoramento contínuo.
:::

## Privacidade e Segurança

- Privacidade:
    - Dados de Treinamento: Modelos (especialmente LLMs) são treinados com grandes volumes de dados, que podem conter informações pessoais ou sensíveis.
    - Risco de Memorização: Modelos podem "memorizar" partes dos dados de treinamento e revelá-las inadvertidamente em suas saídas.

## Privacidade e Segurança

- Privacidade:
    - Ataques de Inferência: Tentativas de extrair informações privadas sobre os dados de treinamento a partir das previsões do modelo.
    - Soluções: Privacidade Diferencial, Aprendizado Federado, Anonimização de dados.

## Privacidade e Segurança

- Segurança:
    - Ataques Adversariais: Pequenas perturbações (imperceptíveis para humanos) na entrada podem fazer o modelo errar completamente (ex: classificar um sinal de "Pare" como "Limite de Velocidade").

## Privacidade e Segurança

- Segurança:
    - Envenenamento de Dados (Data Poisoning): Inserir dados maliciosos no conjunto de treinamento para corromper o modelo.
    - Roubo de Modelo: Extrair a funcionalidade de um modelo proprietário através de consultas.

::: notes
A IA também levanta questões significativas de privacidade e segurança. Como garantir que dados sensíveis usados no treinamento não sejam expostos? Como proteger os modelos contra ataques maliciosos que visam enganá-los ou roubá-los? Técnicas como Privacidade Diferencial buscam proteger a privacidade individual nos dados, enquanto a pesquisa em segurança se concentra em tornar os modelos mais robustos contra ataques adversariais e outras ameaças.
:::

## Generalização e Robustez dos Modelos

- Generalização: A capacidade do modelo de performar bem em dados novos e não vistos durante o treinamento. Um modelo que apenas memoriza os dados de treino (overfitting) não generaliza bem.

## Generalização e Robustez dos Modelos


- Robustez: A capacidade do modelo de manter seu desempenho mesmo quando confrontado com:
    - Pequenas Perturbações: Como nos ataques adversariais.
    - Mudanças na Distribuição dos Dados (Domain Shift): O mundo real muda, e os dados podem começar a parecer diferentes daqueles com os quais o modelo foi treinado (ex: um modelo treinado no verão pode performar mal no inverno).

## Generalização e Robustez dos Modelos

- Desafio: Modelos atuais, apesar de poderosos, podem ser frágeis e sensíveis a pequenas mudanças ou a dados ligeiramente fora da distribuição de treinamento.

::: notes
Um bom modelo de ML não deve apenas acertar nos dados que viu, mas também generalizar para novos dados. O overfitting (ajuste excessivo aos dados de treino) é um problema clássico. Além disso, a robustez é crucial. Queremos modelos que não sejam facilmente enganados por pequenas alterações na entrada e que continuem funcionando razoavelmente bem mesmo quando os dados do mundo real mudam um pouco em relação aos dados de treinamento. Garantir generalização e robustez ainda são áreas ativas de pesquisa.
:::



## Futuro do Machine Learning e IA Generativa

- Fundamentos Permanecem
- Convergência e Hibridização
- Multimodalidade
- Eficiência
- Raciocínio, Planejamento e Ferramentas
- PINNs

::: notes

Fundamentos Permanecem: ML Clássico (supervisionado, não-supervisionado) e Deep Learning (CNNs, RNNs, etc.) continuam sendo a base sobre a qual a IA Generativa é construída. Entender esses fundamentos é crucial.
Convergência e Hibridização: Veremos mais modelos que combinam técnicas (ex: LLMs usando ferramentas de ML clássico, modelos generativos para data augmentation em tarefas supervisionadas).
Multimodalidade: Modelos que entendem e geram múltiplos tipos de dados simultaneamente (texto, imagem, áudio, vídeo) como Gemini, GPT-4V.
Eficiência: Foco em modelos menores, mais rápidos e energeticamente eficientes (quantização, destilação, arquiteturas eficientes).
Raciocínio, Planejamento e Ferramentas: LLMs evoluindo para além da geração de texto, incorporando capacidades de raciocínio mais complexas, planejamento e uso de ferramentas externas (calculadoras, APIs, busca na web, execução de código) -> Agentes.
Personalização Extrema: IA adaptada às necessidades e contextos individuais.
Governança e Ética: Necessidade crescente de frameworks robustos para governança, ética e segurança em IA.
::: 




## Arquiteturas

-   [Flamingo](https://arxiv.org/abs/2204.14198)

-   [Perceiver IO](https://arxiv.org/abs/2107.14795)

-   [NVLM](https://arxiv.org/abs/2409.11402)

::: notes
Vamos olhar brevemente para algumas arquiteturas e técnicas mais avançadas que estão impulsionando a IA Generativa, particularmente em torno dos LLMs e da multimodalidade.
:::

## RAG

- Problema: LLMs "puros" podem alucinar (inventar fatos) e seu conhecimento é limitado aos dados de treinamento (podem estar desatualizados).
- Solução RAG: Combina um LLM pré-treinado com um mecanismo de busca/recuperação externo.

## RAG

- Processo:
    - Usuário faz uma pergunta.
    - A pergunta é usada para buscar documentos/trechos relevantes em uma base de conhecimento externa (ex: artigos, documentos internos, web).
    - Os trechos recuperados são adicionados ao prompt original do usuário.
    - O LLM gera a resposta baseado no prompt aumentado (pergunta + contexto recuperado).

## RAG


- Benefícios: Reduz alucinações, permite usar conhecimento atualizado ou específico de um domínio, possibilita citar fontes.
- Componentes Chave: Retriever (busca vetorial é comum), LLM.

::: notes
RAG (Retrieval-Augmented Generation) é uma técnica poderosa para melhorar a confiabilidade e a atualidade das respostas dos LLMs. Em vez de depender apenas do conhecimento interno do modelo (que pode ser limitado ou desatualizado), o RAG primeiro busca informações relevantes em uma base de dados externa (usando técnicas de busca, muitas vezes baseadas em embeddings e vector stores). Essa informação recuperada é então fornecida ao LLM junto com a pergunta original, dando-lhe o contexto necessário para gerar uma resposta mais precisa e fundamentada. Isso é crucial para aplicações empresariais onde a precisão e a capacidade de citar fontes são importantes.
:::

## Perguntas

![QR Code](imgs/qrcode.png)
